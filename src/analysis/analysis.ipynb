{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import re\n",
    "from sys import exit\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# globals\n",
    "IGNORE_ANDROID_10 = True  # toggle to use extra android 10 data feature points or not\n",
    "ANDROID_10_FEATURES = ('lastTimeForegroundServiceUsed', 'lastTimeVisible',\n",
    "                      'totalTimeForegroundServiceUsed', 'totalTimeVisible')  # the android 10 data feature points\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from mariadb table\n",
    "def read_data(com):\n",
    "    con = 'mysql+mysqlconnector://admin:password@127.0.0.1:3306/Dissertation'\n",
    "    return pd.read_sql_table(com, con=con)\n",
    "\n",
    "# load data\n",
    "call_df = read_data('calls')\n",
    "user_df = read_data('user')\n",
    "category_df = read_data('app_categories')\n",
    "location_df = read_data('locations')\n",
    "session_df = read_data('user_session_data')\n",
    "\n",
    "# remove the id column that comes from the database. isn't necessary\n",
    "call_df = call_df.drop(columns='id')\n",
    "location_df = location_df.drop(columns='id')\n",
    "session_df = session_df.drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.    \"\"\"\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "#         print(data_dict)\n",
    "#         print(self.key)\n",
    "        return data_dict[self.key]\n",
    "    \n",
    "\n",
    "class SparseTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "  \n",
    "    def transform(self, X):\n",
    "        csr = csr_matrix(X)\n",
    "#         print(csr)\n",
    "        return csr\n",
    "\n",
    "\n",
    "class TransposeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "  \n",
    "    def transform(self, X):\n",
    "        return X.transpose().toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoder\n",
    "category_encoder = LabelEncoder()\n",
    "category_labels = category_encoder.fit_transform(category_df['category'])\n",
    "category_mappings = {label: index for index, label in enumerate(category_encoder.classes_)}\n",
    "\n",
    "# one hot it\n",
    "category_one_hot = OneHotEncoder()\n",
    "category_feature = category_one_hot.fit_transform(category_df[['category']]).toarray()\n",
    "category_feature_labels = list(category_encoder.classes_)\n",
    "\n",
    "app_encoder = LabelEncoder()\n",
    "app_encoder_labels = app_encoder.fit_transform(category_df['app_name'])\n",
    "app_mappings = {label: index for index, label in enumerate(app_encoder.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data converters\n",
    "def convert_session_app_data(string):\n",
    "#     takes in a dictionary as a string and convert it to dictionary\n",
    "    obj = dict()\n",
    "    pattern = re.compile(r'[\\w]+=[\\w ]+')\n",
    "    matches = pattern.findall(string)\n",
    "    name = ''\n",
    "    for match in matches:\n",
    "        split_match = match.split('=')\n",
    "        if split_match[0] == 'name':  # name is the only key that does not have an integer value\n",
    "#             obj[split_match[0]] = app_mappings.get(split_match[1])\n",
    "            name = split_match[1]\n",
    "        elif split_match[0] == 'category':\n",
    "            obj[split_match][0] = int(category_mappings.get(split_match[1], -1))\n",
    "        else:\n",
    "            if IGNORE_ANDROID_10 and split_match[0] in ANDROID_10_FEATURES:\n",
    "                continue\n",
    "            obj[split_match[0]] = int(split_match[1])\n",
    "#     print(name)\n",
    "    if name != '':\n",
    "        try:\n",
    "            obj['category'] = int(category_mappings.get(category_df[category_df['app_name']==name]['category'].values[0], -1))\n",
    "        except:\n",
    "            obj['category'] = int(-1)\n",
    "    else:\n",
    "        obj['category'] = int(-1)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def convert_session_data_list(session_data):\n",
    "#     convert list of dictionary strings to list of proper dictionary objects\n",
    "    obj = list()\n",
    "    pattern = re.compile(r'(\\{[A-Za-z0-9_=, ]+\\})')\n",
    "    matches = pattern.findall(session_data)\n",
    "    for match in matches:\n",
    "        obj.append(convert_session_app_data(match))\n",
    "    return obj\n",
    "\n",
    "\n",
    "# data addition functions\n",
    "def add_sias_score(uid):\n",
    "    return int(user_df.loc[user_df['uid'] == uid].values[0][1])\n",
    "\n",
    "\n",
    "def add_category(session_data):\n",
    "#     for each dictionary in the list, find the package and category for that app name, append to dict, and save new dict to list\n",
    "    updated_data = list()\n",
    "    for item in session_data:\n",
    "        app_name = item.get('name', None)\n",
    "        if app_name is None:\n",
    "            print('app name is none for data: ' + item)\n",
    "            continue  # shouldn't happen but just in case\n",
    "        \n",
    "        app = category_df.loc[category_df['app_name'] == app_name].values\n",
    "        app_category = None\n",
    "        \n",
    "        # TODO: fix this once all data is collected. the problem is due to not all new data added yet\n",
    "        try:\n",
    "            app_category = app[0][1]\n",
    "        except IndexError as e:\n",
    "            continue\n",
    "        \n",
    "        item['app_category'] = category_mappings.get(app_category, app_category)\n",
    "        updated_data.append(item)\n",
    "    \n",
    "    return updated_data\n",
    "\n",
    "    \n",
    "# TODO: add location latitude and longitude that pertains to that session\n",
    "\n",
    "\n",
    "# add total session length\n",
    "def add_session_length(row):\n",
    "    return int(row['session_end']) - int(row['session_start'])\n",
    "\n",
    "\n",
    "# add social quantifier\n",
    "def add_social_quantifier(sias):\n",
    "    if sias >= 43:\n",
    "        return 2\n",
    "    elif sias <= 33:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert session data string to actual python object\n",
    "session_df['session_data'] = session_df.session_data.apply(convert_session_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add session length\n",
    "# session_df['session_length'] = session_df.apply(add_session_length, axis=1)\n",
    "# # add category and package name to each app in each session object\n",
    "# session_df['session_data'] = session_df.session_data.apply(add_category)\n",
    "# # add sias score to each session\n",
    "session_df['sias'] = session_df.uid.apply(add_sias_score)\n",
    "# # add string quantifier\n",
    "# session_df['level'] = session_df.sias.apply(add_social_quantifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all that shit and flatten the living fuck out of it\n",
    "# uid, sias, app name, category, session_interval, app_total_time_used, app\n",
    "# combined_data = pd.DataFrame(columns=['uid', 'sias', 'app_name', 'category', 'session_interval', 'last_time_used', 'total_time_foreground'])\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "def combine(row):\n",
    "    global combined_data\n",
    "    sias = row['sias']\n",
    "    uid = row['uid']\n",
    "    session_start = int(row['session_start'])\n",
    "    session_end = int(row['session_end'])\n",
    "    session_interval = pd.Interval(left=int(row['session_start']), right=int(row['session_end']), closed='both')\n",
    "    for session in row['session_data']:\n",
    "#         app_name = session.get('name')\n",
    "        last_time_used = session.get('lastTimeUsed')\n",
    "        total_time_in_foreground = session.get('totalTimeInForeground')\n",
    "        app_category = session.get('category')\n",
    "        frame = pd.DataFrame([uid, sias, app_category, session_start, session_end, last_time_used, total_time_in_foreground, session_interval])\n",
    "        combined_data = pd.concat([combined_data, frame], axis=1, ignore_index=True)\n",
    "#         print(frame)\n",
    "#         break\n",
    "    \n",
    "session_df.apply(combine, axis=1)\n",
    "combined_data = combined_data.T\n",
    "combined_data = combined_data.rename(columns={\n",
    "    0: 'uid',\n",
    "    1: 'sias',\n",
    "#     2: 'app_name',\n",
    "    2: 'category',\n",
    "    3: 'session_start',\n",
    "    4: 'session_end',\n",
    "    5: 'last_time_used',\n",
    "    6: 'total_time_foreground',\n",
    "    7: 'session_interval'\n",
    "})\n",
    "\n",
    "combined_data['uid'] = combined_data['uid'].astype('string')\n",
    "combined_data['sias'] = combined_data['sias'].astype('int')\n",
    "combined_data['session_interval'] = combined_data['session_interval'].astype('interval')\n",
    "combined_data['category'] = combined_data['category'].astype('int')\n",
    "combined_data['session_start'] = combined_data['session_start'].astype('int')\n",
    "combined_data['session_end'] = combined_data['session_end'].astype('int')\n",
    "combined_data['last_time_used'] = combined_data['last_time_used'].astype('int')\n",
    "combined_data['total_time_foreground'] = combined_data['total_time_foreground'].astype('int')\n",
    "\n",
    "combined_data = combined_data.drop(columns='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sias</th>\n",
       "      <th>category</th>\n",
       "      <th>session_start</th>\n",
       "      <th>session_end</th>\n",
       "      <th>last_time_used</th>\n",
       "      <th>total_time_foreground</th>\n",
       "      <th>session_interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>21</td>\n",
       "      <td>1582906131929</td>\n",
       "      <td>1582906535913</td>\n",
       "      <td>1582839465286</td>\n",
       "      <td>659</td>\n",
       "      <td>[1582906131929, 1582906535913]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sias  category  session_start    session_end  last_time_used  \\\n",
       "0    44        21  1582906131929  1582906535913   1582839465286   \n",
       "\n",
       "   total_time_foreground                session_interval  \n",
       "0                    659  [1582906131929, 1582906535913]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_threads = combined_data.sample(frac=1)\n",
    "\n",
    "# validation_split = int(len(random_threads) * 0.8)\n",
    "# train_data = random_threads.iloc[:validation_split, :]\n",
    "# validation_data = random_threads.iloc[validation_split:, :]\n",
    "\n",
    "# train_labels = train_data[['sias']]\n",
    "\n",
    "# validation_labels = validation_data['sias']\n",
    "\n",
    "# session_train = train_data[['category', 'last_time_used', 'total_time_foreground']]\n",
    "session_train, session_test, train_labels, test_labels = train_test_split(combined_data, combined_data['sias'], test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 1, expected 10497.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7162641c3f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m ])\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/level4dissertation/src/analysis/venv/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/Documents/level4dissertation/src/analysis/venv/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/level4dissertation/src/analysis/venv/lib/python3.7/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/level4dissertation/src/analysis/venv/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/level4dissertation/src/analysis/venv/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m             \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/level4dissertation/src/analysis/venv/lib/python3.7/site-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \"\"\"\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/level4dissertation/src/analysis/venv/lib/python3.7/site-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    584\u001b[0m                                                     \u001b[0mexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrow_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                                                     got=A.shape[0]))\n\u001b[0;32m--> 586\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbcol_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 1, expected 10497."
     ]
    }
   ],
   "source": [
    "# c = DecisionTreeClassifier()\n",
    "# c = c.fit(session_train, train_labels)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "      transformer_list=[\n",
    "          ('category', Pipeline([\n",
    "              ('selector', ItemSelector(key='category')),\n",
    "              ('sparse', SparseTransformer()),\n",
    "              ('transform', TransposeTransformer()),\n",
    "              ('onehot', OneHotEncoder())\n",
    "          ])),\n",
    "          ('session_length', Pipeline([\n",
    "              ('selector', ItemSelector(key='session_interval')),\n",
    "#               ('sparse', SparseTransformer()),\n",
    "#               ('transform', TransposeTransformer()),\n",
    "#               ('onehot', OneHotEncoder())\n",
    "          ])),\n",
    "      ])),\n",
    "    ('tree', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "result = pipeline.fit(session_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
