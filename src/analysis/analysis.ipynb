{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T19:07:07.050556Z",
     "start_time": "2020-03-21T19:07:06.567953Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import re\n",
    "from sys import exit\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# globals\n",
    "IGNORE_ANDROID_10 = True  # toggle to use extra android 10 data feature points or not\n",
    "ANDROID_10_FEATURES = ('lastTimeForegroundServiceUsed', 'lastTimeVisible',\n",
    "                       'totalTimeForegroundServiceUsed', 'totalTimeVisible'\n",
    "                       )  # the android 10 data feature points\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T19:07:07.992835Z",
     "start_time": "2020-03-21T19:07:07.052410Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# read data from mariadb table\n",
    "def read_data(com):\n",
    "    con = 'mysql+mysqlconnector://admin:password@127.0.0.1:3306/Dissertation'\n",
    "    return pd.read_sql_table(com, con=con)\n",
    "\n",
    "\n",
    "# load data\n",
    "call_df = read_data('calls')\n",
    "user_df = read_data('user')\n",
    "category_df = read_data('app_categories')\n",
    "location_df = read_data('locations')\n",
    "session_df = read_data('user_session_data')\n",
    "\n",
    "# remove the id column that comes from the database. isn't necessary\n",
    "call_df = call_df.drop(columns='id')\n",
    "location_df = location_df.drop(columns='id')\n",
    "session_df = session_df.drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T19:07:08.015443Z",
     "start_time": "2020-03-21T19:07:07.994937Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        #         print(data_dict)\n",
    "        #         print(self.key)\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class SparseTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        csr = csr_matrix(X)\n",
    "        #         print(csr)\n",
    "        return csr\n",
    "\n",
    "\n",
    "class TransposeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T19:27:08.400399Z",
     "start_time": "2020-03-21T19:27:08.379938Z"
    }
   },
   "outputs": [],
   "source": [
    "# label encoder\n",
    "category_encoder = LabelEncoder()\n",
    "category_labels = category_encoder.fit_transform(category_df['category'])\n",
    "category_mappings = {label: index for index, label in enumerate(category_encoder.classes_)}\n",
    "\n",
    "# one hot it\n",
    "# category_one_hot = OneHotEncoder()\n",
    "# category_feature = category_one_hot.fit_transform(category_df[['category']]).toarray()\n",
    "# category_feature_labels = list(category_encoder.classes_)\n",
    "\n",
    "# category_feature_labels.index(category_df['category'].iloc(0)[1])\n",
    "# category_df[category_df['app_name'].str.match('2048')]['category'].iloc[0]\n",
    "# category_feature\n",
    "\n",
    "app_encoder = LabelEncoder()\n",
    "app_encoder_labels = app_encoder.fit_transform(category_df['app_name'])\n",
    "app_mappings = {label: index for index, label in enumerate(app_encoder.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T19:36:40.953399Z",
     "start_time": "2020-03-21T19:36:40.927481Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# data converters\n",
    "def convert_session_app_data(string):\n",
    "    #     takes in a dictionary as a string and convert it to dictionary\n",
    "    obj = dict()\n",
    "    pattern = re.compile(r'[\\w]+=[\\w ]+')\n",
    "    matches = pattern.findall(string)\n",
    "    name = ''\n",
    "    for match in matches:\n",
    "        split_match = match.split('=')\n",
    "        if split_match[0] in [\n",
    "                'name', 'category', 'app_name'\n",
    "        ]:  # name is the only key that does not have an integer value\n",
    "            #             obj[split_match[0]] = app_mappings.get(split_match[1])\n",
    "            obj[split_match[0]] = str(split_match[1])\n",
    "#         elif split_match[0] == 'category':\n",
    "#             obj[split_match][0] = int(category_mappings.get(split_match[1], -1))\n",
    "        else:\n",
    "            if IGNORE_ANDROID_10 and split_match[0] in ANDROID_10_FEATURES:\n",
    "                continue\n",
    "            obj[split_match[0]] = int(split_match[1])\n",
    "\n",
    "\n",
    "#     print(name)\n",
    "#     if name != '':\n",
    "#         try:\n",
    "#             obj['category'] = int(category_mappings.get(category_df[category_df['app_name']==name]['category'].values[0], -1))\n",
    "#         except:\n",
    "#             obj['category'] = int(-1)\n",
    "#     else:\n",
    "#         obj['category'] = int(-1)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def convert_session_data_list(session_data):\n",
    "    #     convert list of dictionary strings to list of proper dictionary objects\n",
    "    obj = list()\n",
    "    pattern = re.compile(r'(\\{[A-Za-z0-9_=, ]+\\})')\n",
    "    matches = pattern.findall(session_data)\n",
    "    for match in matches:\n",
    "        obj.append(convert_session_app_data(match))\n",
    "    return obj\n",
    "\n",
    "\n",
    "# data addition functions\n",
    "def add_sias_score(uid):\n",
    "    return int(user_df.loc[user_df['uid'] == uid].values[0][1])\n",
    "\n",
    "\n",
    "def add_category(session_data):\n",
    "    #     for each dictionary in the list, find the package and category for that app name, append to dict, and save new dict to list\n",
    "    updated_data = list()\n",
    "    for item in session_data:\n",
    "        app_name = item.get('name', None)\n",
    "        if app_name is None:\n",
    "            print('app name is none for data: ' + item)\n",
    "            continue  # shouldn't happen but just in case\n",
    "\n",
    "        app = category_df.loc[category_df['app_name'] == app_name].values\n",
    "        app_category = None\n",
    "\n",
    "        # TODO: fix this once all data is collected. the problem is due to not all new data added yet\n",
    "        try:\n",
    "            app_category = app[0][1]\n",
    "        except IndexError as e:\n",
    "            continue\n",
    "\n",
    "        item['app_category'] = category_mappings.get(app_category)\n",
    "        updated_data.append(item)\n",
    "\n",
    "    return updated_data\n",
    "\n",
    "\n",
    "# TODO: add location latitude and longitude that pertains to that session\n",
    "\n",
    "\n",
    "# add total session length\n",
    "def add_session_length(row):\n",
    "    return int(row['session_end']) - int(row['session_start'])\n",
    "\n",
    "\n",
    "# add social quantifier\n",
    "def add_social_quantifier(sias):\n",
    "    if sias >= 43:\n",
    "        return 2\n",
    "    elif sias <= 33:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T19:07:09.402348Z",
     "start_time": "2020-03-21T19:07:08.046204Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# convert session data string to actual python object\n",
    "session_df['session_data'] = session_df.session_data.apply(convert_session_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T19:08:15.706219Z",
     "start_time": "2020-03-21T19:07:09.403759Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# add session length\n",
    "# session_df['session_length'] = session_df.apply(add_session_length, axis=1)\n",
    "# # add category and package name to each app in each session object\n",
    "session_df['session_data'] = session_df.session_data.apply(add_category)\n",
    "# # add sias score to each session\n",
    "session_df['sias'] = session_df.uid.apply(add_sias_score)\n",
    "# # add string quantifier\n",
    "# session_df['level'] = session_df.sias.apply(add_social_quantifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T19:33:53.910308Z",
     "start_time": "2020-03-21T19:33:53.459416Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uid, sias, app name, category, session_interval, app_total_time_used, app\n",
    "# combined_data = pd.DataFrame(columns=['uid', 'sias', 'app_name', 'category', 'session_interval', 'last_time_used', 'total_time_foreground'])\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "def combine(row):\n",
    "    global combined_data\n",
    "    sias = row['sias']\n",
    "    #     uid = row['uid']\n",
    "    session_start = int(row['session_start'])\n",
    "    session_end = int(row['session_end'])\n",
    "    session_length = int(session_end - session_start)\n",
    "    \n",
    "    frame = pd.DataFrame.from_records(row['session_data'])\n",
    "    frame['session_start'] = session_start\n",
    "    frame['session_end'] = session_end\n",
    "    frame['session_length'] = session_length\n",
    "    frame['sias'] = sias\n",
    "    combined_data = pd.concat([combined_data, frame], axis=0, ignore_index=True)\n",
    "    \n",
    "#     for session in row['session_data']:\n",
    "#         frame = pd.DataFrame.from_records([session])\n",
    "#         print(frame)\n",
    "#         print()\n",
    "#         break\n",
    "#         app_name = session.get('name')\n",
    "#         last_time_used = session.get('lastTimeUsed')\n",
    "#         total_time_in_foreground = session.get('totalTimeInForeground')\n",
    "#         app_category = session.get('app_category')\n",
    "#         frame = pd.DataFrame([\n",
    "#             sias, app_name, app_category, session_start, session_end,\n",
    "#             last_time_used, total_time_in_foreground, session_length\n",
    "#         ])\n",
    "#         combined_data = pd.concat([combined_data, frame],\n",
    "#                                   axis=1,\n",
    "#                                   ignore_index=True)\n",
    "\n",
    "\n",
    "#         print(frame)\n",
    "#         break\n",
    "\n",
    "session_df.sample(frac=0.01).apply(combine, axis=1)\n",
    "# combined_data = combined_data.T\n",
    "# combined_data = combined_data.rename(\n",
    "#     columns={\n",
    "#         #     0: 'uid',\n",
    "#         0: 'sias',\n",
    "#         1: 'app_name',\n",
    "#         2: 'category',\n",
    "#         3: 'session_start',\n",
    "#         4: 'session_end',\n",
    "#         5: 'last_time_used',\n",
    "#         6: 'total_time_foreground',\n",
    "#         7: 'session_length'\n",
    "#     })\n",
    "\n",
    "# # combined_data['uid'] = combined_data['uid'].astype('string')\n",
    "# combined_data['sias'] = combined_data['sias'].astype('int')\n",
    "# combined_data['session_length'] = combined_data['session_length'].astype('int')\n",
    "# combined_data['category'] = combined_data['category'].astype('str')\n",
    "# combined_data['session_start'] = combined_data['session_start'].astype('int')\n",
    "# combined_data['session_end'] = combined_data['session_end'].astype('int')\n",
    "# combined_data['last_time_used'] = combined_data['last_time_used'].astype('int')\n",
    "# combined_data['total_time_foreground'] = combined_data[\n",
    "#     'total_time_foreground'].astype('int')\n",
    "\n",
    "# combined_data = combined_data.drop(columns='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T19:34:41.527744Z",
     "start_time": "2020-03-21T19:34:41.499599Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T19:05:59.973792Z",
     "start_time": "2020-03-21T19:05:59.910783Z"
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "oh = OneHotEncoder(dtype=np.str, sparse=True)\n",
    "\n",
    "combined_data['app_name'] = le.fit_transform(combined_data['app_name'])\n",
    "combined_data = pd.get_dummies(combined_data)\n",
    "# combined_data['category'] = oh.fit_transform(combined_data['category'].to_numpy().reshape(len(combined_data['category']),1)).T\n",
    "\n",
    "combined_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-21T17:52:09.709Z"
    }
   },
   "outputs": [],
   "source": [
    "# random_threads = combined_data.sample(frac=1)\n",
    "\n",
    "# validation_split = int(len(random_threads) * 0.8)\n",
    "# train_data = random_threads.iloc[:validation_split, :]\n",
    "# validation_data = random_threads.iloc[validation_split:, :]\n",
    "\n",
    "# train_labels = train_data[['sias']]\n",
    "\n",
    "# validation_labels = validation_data['sias']\n",
    "\n",
    "# session_train = train_data[['category', 'last_time_used', 'total_time_foreground']]\n",
    "session_train, session_test, train_labels, test_labels = train_test_split(combined_data, combined_data['sias'], test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T19:19:35.576456Z",
     "start_time": "2020-03-21T19:19:35.541473Z"
    }
   },
   "outputs": [],
   "source": [
    "session_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-21T17:52:12.270Z"
    }
   },
   "outputs": [],
   "source": [
    "c = DecisionTreeClassifier()\n",
    "c = c.fit(session_train, train_labels)\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('union', FeatureUnion(\n",
    "#       transformer_list=[\n",
    "# #           ('category', Pipeline([\n",
    "# #               ('selector', ItemSelector(key='category')),\n",
    "# #               ('sparse', SparseTransformer()),\n",
    "# #               ('transform', TransposeTransformer()),\n",
    "# #               ('onehot', LabelEncoder())\n",
    "# #           ])),\n",
    "# #           ('session_length', Pipeline([\n",
    "# #               ('selector', ItemSelector(key='session_length')),\n",
    "# # #               ('sparse', SparseTransformer()),\n",
    "# # #               ('transform', TransposeTransformer()),\n",
    "# # #               ('onehot', OneHotEncoder())\n",
    "# #           ])),\n",
    "#       ])),\n",
    "#     ('tree', DecisionTreeClassifier())\n",
    "# ])\n",
    "\n",
    "# result = pipeline.fit(pd.get_dummies(session_train), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:50:35.679450Z",
     "start_time": "2020-03-21T17:50:35.657814Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
